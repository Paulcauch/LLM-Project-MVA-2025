{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extend model length with LongRope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to technical limitation, we can not apply it to a big pre train LLM. Here we just demonstrate how the code works. We start from a vanilla LLM (even tho the whole point of this paper is to start from a **pre-trained one**) where we suppose that its pre trained length is set to 1048 and we want to extend it to 2048. This notebook is just a demonstration on how to use the functions in ``src`` we'll do several simplification for the code to run quickly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from src.dataset import TextDataset\n",
    "from src.utils_data import load_data\n",
    "from src.utils_general import truncate_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (338025 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.model_max_length = 2048\n",
    "\n",
    "# We use this tokenizer just to convert the inputs to ids, basically any other tokenizer would work\n",
    "# since we don't use the vector representation but only the id. The embedding is learned by the model\n",
    "\n",
    "tensor_list = load_data(\"../data/input.txt\", tokenizer, tokenizer.model_max_length)\n",
    "\n",
    "tensor_list = truncate_ids(tensor_list, 5000)\n",
    "\n",
    "dataset = TextDataset(tensor_list)\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.longrope import LongRoPEModel\n",
    "\n",
    "model = LongRoPEModel(\n",
    "    d_model=256,\n",
    "    n_heads=32,\n",
    "    num_layers=6,\n",
    "    vocab_size=5000,\n",
    "    max_len=1048, # max_len is 1048 is the \"base max_len\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "searching for lambda factors: 100%|██████████| 2/2 [04:15<00:00, 127.67s/it]\n",
      "fine tuning step:   0%|          | 1/200 [00:08<27:30,  8.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Validation Perplexity: 3015.17919921875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fine tuning step:  26%|██▌       | 51/200 [00:54<07:53,  3.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 50, Validation Perplexity: 207.99923706054688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fine tuning step:  50%|█████     | 101/200 [01:40<05:14,  3.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100, Validation Perplexity: 117.55162811279297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fine tuning step:  76%|███████▌  | 151/200 [02:26<02:35,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 150, Validation Perplexity: 94.0124740600586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fine tuning step: 100%|██████████| 200/200 [03:02<00:00,  1.09it/s]\n"
     ]
    }
   ],
   "source": [
    "model_2 = model.extend_context(\n",
    "    dataset=tensor_list,\n",
    "    target_length=1048*2, # here the scale factor s is 2\n",
    "    max_sequence_length=1048,\n",
    "    tokenizer=tokenizer,\n",
    "    population_size=2,\n",
    "    num_mutations=1,\n",
    "    num_crossovers=1,\n",
    "    max_iterations=2,\n",
    ") # the code here is to adapt a model that is pre trained with 1048 length inputs and we want to reach 2048\n",
    "# we actually only do 1 step (extend to 1048 to 2096)\n",
    "# we take the minimum parameters in order to check if the model runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2048])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 2.1492, -0.1572,  0.0369,  ...,  0.7049,  0.0450, -0.2447],\n",
       "          [ 0.4464, -1.6841, -1.1774,  ..., -0.9527,  1.0362, -0.8891],\n",
       "          [ 0.2646,  1.3279, -0.1183,  ..., -0.0783, -0.2850, -0.2472],\n",
       "          ...,\n",
       "          [-0.2188,  1.3719, -1.3156,  ...,  0.6452,  0.6182,  0.0043],\n",
       "          [-0.9520, -0.9932,  0.1300,  ..., -0.3708, -0.0574,  0.6455],\n",
       "          [ 1.6752, -1.4626, -0.0169,  ...,  1.8839, -0.2566, -0.0997]]],\n",
       "        device='cuda:0', grad_fn=<AddBackward0>),\n",
       " (tensor([[[ 1.6492, -0.1572, -0.4631,  ...,  0.7049, -0.1550, -0.2447],\n",
       "           [ 0.1763, -2.1048, -1.4475,  ..., -0.9734,  0.8373, -0.9098],\n",
       "           [ 0.4727,  0.8733,  0.0898,  ..., -0.1195, -0.4807, -0.2884],\n",
       "           ...,\n",
       "           [-0.0147,  0.9155, -1.1115,  ...,  0.4455,  0.6069, -0.1954],\n",
       "           [-0.4576, -1.0681,  0.6243,  ..., -0.5706, -0.0480,  0.4457],\n",
       "           [ 2.0054, -1.0870,  0.3133,  ...,  1.6862, -0.2266, -0.2975]]],\n",
       "         device='cuda:0', grad_fn=<SliceBackward0>),\n",
       "  tensor([[[ 0.5000,  0.0000,  0.5000,  ...,  0.0000,  0.2000,  0.0000],\n",
       "           [ 0.2702,  0.4207,  0.2702,  ...,  0.0207,  0.1989,  0.0207],\n",
       "           [-0.2081,  0.4546, -0.2081,  ...,  0.0412,  0.1957,  0.0412],\n",
       "           ...,\n",
       "           [-0.2041,  0.4565, -0.2041,  ...,  0.1997,  0.0114,  0.1997],\n",
       "           [-0.4944,  0.0749, -0.4944,  ...,  0.1998, -0.0094,  0.1998],\n",
       "           [-0.3301, -0.3755, -0.3301,  ...,  0.1977, -0.0300,  0.1977]]],\n",
       "         device='cuda:0')))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.only_embeddings(dataset[4][0].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(25, device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(dataset[4][0][:-1].unsqueeze(0))[:,-1,:].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(25)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[4][0][-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
