{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extend model length with LongRope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to technical limitation, we can not apply it to a big pre train LLM. Here we just demonstrate how the code works. We start from a vanilla LLM (even tho the whole point of this paper is to start from a **pre-trained one**) where we suppose that its pre trained length is set to 1048 and we want to extend it to 2048. This notebook is just a demonstration on how to use the functions in ``src`` we'll do several simplification for the code to run quickly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from src.dataset import TextDataset\n",
    "from src.utils_data import load_data\n",
    "from src.utils_general import truncate_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (338025 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.model_max_length = 2048\n",
    "\n",
    "# We use this tokenizer just to convert the inputs to ids, basically any other tokenizer would work\n",
    "# since we don't use the vector representation but only the id. The embedding is learned by the model\n",
    "\n",
    "tensor_list = load_data(\"../data/input.txt\", tokenizer, tokenizer.model_max_length)\n",
    "\n",
    "tensor_list = truncate_ids(tensor_list, 5000)\n",
    "\n",
    "dataset = TextDataset(tensor_list)\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.longrope import LongRoPEModel\n",
    "\n",
    "model = LongRoPEModel(\n",
    "    d_model=256,\n",
    "    n_heads=32,\n",
    "    num_layers=6,\n",
    "    vocab_size=5000,\n",
    "    max_len=2048, # max_len is 2048, meaning a model can not take input of higher length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "searching for lambda factors: 100%|██████████| 2/2 [01:32<00:00, 46.21s/it]\n",
      "fine tuning step:   1%|          | 2/200 [00:03<04:24,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Validation Perplexity: 3458.436767578125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fine tuning step:  26%|██▌       | 52/200 [00:21<02:06,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 50, Validation Perplexity: 200.75770568847656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fine tuning step:  50%|█████     | 101/200 [00:39<01:56,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100, Validation Perplexity: 115.11137390136719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fine tuning step:  76%|███████▌  | 151/200 [00:57<00:57,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 150, Validation Perplexity: 94.21158599853516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fine tuning step: 100%|██████████| 200/200 [01:12<00:00,  2.78it/s]\n"
     ]
    }
   ],
   "source": [
    "model_2 = model.extend_context(\n",
    "    dataset=tensor_list,\n",
    "    target_length=1048*2,\n",
    "    max_sequence_length=2048,\n",
    "    tokenizer=tokenizer,\n",
    "    population_size=2,\n",
    "    num_mutations=1,\n",
    "    num_crossovers=1,\n",
    "    max_iterations=2,\n",
    ") # the code here is to adapt a model that is pre trained with 1048 length inputs and we want to reach 2048\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.1254, -0.1966,  0.7539,  ..., -0.5824,  0.9575,  0.0959],\n",
       "          [-0.3238,  0.6256,  0.3047,  ..., -0.5419,  0.9554,  0.1364],\n",
       "          [-1.7411, -0.0195, -0.0610,  ...,  1.0606,  2.0380, -0.0664],\n",
       "          ...,\n",
       "          [ 1.1909, -0.1648, -0.1538,  ..., -2.7744, -2.2751, -0.3570],\n",
       "          [ 0.6313, -2.5097,  0.5689,  ..., -0.1238, -0.9221, -0.5455],\n",
       "          [ 2.1247, -1.4082,  0.4335,  ..., -0.6183, -0.0743, -2.0550]]],\n",
       "        device='cuda:0', grad_fn=<AddBackward0>),\n",
       " (tensor([[[-0.8517, -0.1966, -0.2232,  ..., -0.5824,  0.5667,  0.0959],\n",
       "           [-0.8517, -0.1966, -0.2232,  ..., -0.5824,  0.5667,  0.0959],\n",
       "           [-1.3345, -0.9080,  0.3456,  ...,  0.9801,  1.6555, -0.1468],\n",
       "           ...,\n",
       "           [ 2.1528, -0.3367,  0.8080,  ..., -2.3844, -2.2493,  0.0330],\n",
       "           [ 1.2956, -1.7932,  1.2332,  ...,  0.2668, -0.9368, -0.1550],\n",
       "           [ 1.8807, -0.4621,  0.1895,  ..., -0.2313, -0.1293, -1.6681]]],\n",
       "         device='cuda:0', grad_fn=<EmbeddingBackward0>),\n",
       "  tensor([[[ 0.9771,  0.0000,  0.9771,  ...,  0.0000,  0.3908,  0.0000],\n",
       "           [ 0.5279,  0.8222,  0.5279,  ...,  0.0404,  0.3887,  0.0404],\n",
       "           [-0.4066,  0.8885, -0.4066,  ...,  0.0805,  0.3825,  0.0805],\n",
       "           ...,\n",
       "           [-0.9619,  0.1719, -0.9619,  ..., -0.3900, -0.0258, -0.3900],\n",
       "           [-0.6643, -0.7165, -0.6643,  ..., -0.3906,  0.0147, -0.3906],\n",
       "           [ 0.2440, -0.9461,  0.2440,  ..., -0.3869,  0.0550, -0.3869]]],\n",
       "         device='cuda:0')))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.only_embeddings(dataset[0][0].unsqueeze(0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
